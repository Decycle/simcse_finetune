{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/decycle/ML/simcse_finetune/env/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/home/decycle/ML/simcse_finetune/env/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaLoRA(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): ParametrizedEmbedding(\n",
       "        250002, 1024, padding_idx=1\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): LoRAParametrization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (token_type_embeddings): ParametrizedEmbedding(\n",
       "        1, 1024\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): LoRAParametrization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "    (emb_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Block(\n",
       "          (mixer): MHA(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): ParametrizedLinearResidual(\n",
       "              in_features=1024, out_features=3072, bias=True\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): LoRAParametrization()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): ParametrizedLinear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): LoRAParametrization()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): ParametrizedLinear(\n",
       "              in_features=1024, out_features=4096, bias=True\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): LoRAParametrization()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (fc2): ParametrizedLinear(\n",
       "              in_features=4096, out_features=1024, bias=True\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): LoRAParametrization()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): ParametrizedLinear(\n",
       "        in_features=1024, out_features=1024, bias=True\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): LoRAParametrization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed6990dfbf249188155f7c2ef8d8951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"toughdata/quora-question-answer-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "questions = dataset[\"question\"]\n",
    "answer = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "def get_embedding(texts, model, task=\"text-matching\", batch_size=128):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in trange(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        result = model.encode(batch, task=task, convert_to_tensor=True)\n",
    "        embeddings.append(result)\n",
    "    return torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(similarity_matrix):\n",
    "    sim_matrix = similarity_matrix.cpu().numpy()\n",
    "    n = sim_matrix.shape[0]\n",
    "\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(n):\n",
    "        row = sim_matrix[i]\n",
    "        ranks = (-row).argsort()  # Rank in descending order\n",
    "        rank_of_diag = np.where(ranks == i)[0][0] + 1  # 1-based rank\n",
    "        reciprocal_ranks.append(1 / rank_of_diag)\n",
    "\n",
    "    return np.mean(reciprocal_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_loss(embeddings):\n",
    "    distance_matrix = torch.pdist(embeddings, p=2).pow(2)\n",
    "    exp_kernel = torch.exp(-2 * distance_matrix)\n",
    "    uniform_loss = torch.log(exp_kernel.mean())\n",
    "    return uniform_loss.item()\n",
    "\n",
    "\n",
    "def get_alignment_loss(embeddings_1, embeddings_2):\n",
    "    return (embeddings_1 - embeddings_2).norm(p=2, dim=1).pow(2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.71it/s]\n",
      "100%|██████████| 160/160 [00:59<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMRR (higher is better): 0.43431357869125675\n",
      "\tUniform Loss (Questions): -2.7598531246185303\n",
      "\tUniform Loss (Answer): -2.7526440620422363\n",
      "\tAlignment Loss: 0.6807968020439148\n",
      "\tTop 1 Accuracy: 0.2777343690395355\n",
      "\tTop 5 Accuracy: 0.637890636920929\n",
      "\tTop 10 Accuracy: 0.7685546875\n",
      "\tTop 20 Accuracy: 0.8353515863418579\n",
      "LoRA Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:21<00:00,  1.84it/s]\n",
      "100%|██████████| 160/160 [01:09<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMRR (higher is better): 0.4183315753429723\n",
      "\tUniform Loss (Questions): -3.756124973297119\n",
      "\tUniform Loss (Answer): -3.5556962490081787\n",
      "\tAlignment Loss: 0.9674510955810547\n",
      "\tTop 1 Accuracy: 0.26835939288139343\n",
      "\tTop 5 Accuracy: 0.6103515625\n",
      "\tTop 10 Accuracy: 0.735156238079071\n",
      "\tTop 20 Accuracy: 0.8003906607627869\n",
      "LoRA Model with Query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:21<00:00,  1.84it/s]\n",
      "100%|██████████| 40/40 [00:50<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMRR (higher is better): 0.42633959762058343\n",
      "\tUniform Loss (Questions): -3.6905629634857178\n",
      "\tUniform Loss (Answer): -3.698300361633301\n",
      "\tAlignment Loss: 0.9925322532653809\n",
      "\tTop 1 Accuracy: 0.275390625\n",
      "\tTop 5 Accuracy: 0.6255859732627869\n",
      "\tTop 10 Accuracy: 0.746874988079071\n",
      "\tTop 20 Accuracy: 0.80859375\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_ranking(similarity_matrix):\n",
    "    rank = torch.argsort(similarity_matrix, dim=1, descending=True)\n",
    "    # find the index of i in ith row\n",
    "    row_indices = torch.arange(rank.size(0)).to(rank.device)\n",
    "    # Compare each element in base_rank with its row index\n",
    "    # row_indices.unsqueeze(1)\n",
    "    comparison = rank == row_indices.unsqueeze(1)\n",
    "    # Find the index where the value is True in each row\n",
    "    positions = comparison.nonzero()[:, 1]\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def evaluate(use_lora=False, use_query_lora=False, test_size=1024):\n",
    "    if use_lora:\n",
    "        if use_query_lora:\n",
    "            questions_embedding = get_embedding(\n",
    "                questions[:test_size], model, task=\"retrieval.query\"\n",
    "            )\n",
    "            answer_embedding = get_embedding(\n",
    "                answer[:test_size], model, task=\"retrieval.passage\"\n",
    "            )\n",
    "        else:\n",
    "            questions_embedding = get_embedding(questions[:test_size], model, task='text-matching')\n",
    "            answer_embedding = get_embedding(answer[:test_size], model, batch_size=32, task='text-matching')\n",
    "    else:\n",
    "        questions_embedding = get_embedding(questions[:test_size], model, task=None)\n",
    "        answer_embedding = get_embedding(answer[:test_size], model, batch_size=32, task=None)\n",
    "\n",
    "    similarity_matrix = torch.matmul(questions_embedding, answer_embedding.T)\n",
    "\n",
    "    print(\"\\tMRR (higher is better):\", mean_reciprocal_rank(similarity_matrix))\n",
    "\n",
    "    uniform_loss_questions = get_uniform_loss(questions_embedding)\n",
    "    uniform_loss_answer = get_uniform_loss(answer_embedding)\n",
    "    alignment_loss = get_alignment_loss(questions_embedding, answer_embedding)\n",
    "\n",
    "    print(\"\\tUniform Loss (Questions):\", uniform_loss_questions)\n",
    "    print(\"\\tUniform Loss (Answer):\", uniform_loss_answer)\n",
    "    print(\"\\tAlignment Loss:\", alignment_loss)\n",
    "\n",
    "    ranking = get_ranking(similarity_matrix)\n",
    "    top1 = (ranking == 0).float().mean().item()\n",
    "    top_5 = (ranking < 5).float().mean().item()\n",
    "    top_10 = (ranking < 10).float().mean().item()\n",
    "    top_20 = (ranking < 20).float().mean().item()\n",
    "\n",
    "    print(\"\\tTop 1 Accuracy:\", top1)\n",
    "    print(\"\\tTop 5 Accuracy:\", top_5)\n",
    "    print(\"\\tTop 10 Accuracy:\", top_10)\n",
    "    print(\"\\tTop 20 Accuracy:\", top_20)\n",
    "\n",
    "test_size = 5120\n",
    "print(\"Base Model\")\n",
    "evaluate(use_lora=False, test_size=test_size)\n",
    "print(\"LoRA Model\")\n",
    "evaluate(use_lora=True, test_size=test_size)\n",
    "print(\"LoRA Model with Query\")\n",
    "evaluate(use_lora=True, use_query_lora=True, test_size=test_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
